{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importación de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import tarfile\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import scipy\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Linear regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cambiar el workdir si se ejecuta el notebook localmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(r\"C:\\Users\\aeveg\\OneDrive\\Documentos\\MASTERS\\Proyecto\\Proyecto de Materia\\ProyectoDesarrolloSoluciones\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obteniendo data de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imdb_reviews(ruta_archivo):\n",
    "    # Lee el archivo CSV y lo guarda en un DataFrame\n",
    "    df = pd.read_csv(ruta_archivo)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras 5 filas del conjunto de datos de entrenamiento de reseñas de IMDB en en español:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In Panic In The Streets Richard Widmark plays ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>If you ask me the first one was really better ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I am a big fan a Faerie Tale Theatre and I've ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I just finished reading a book about Dillinger...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Greg Davis and Bryan Daly take some crazed sta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In Panic In The Streets Richard Widmark plays ...          1\n",
       "1  If you ask me the first one was really better ...          0\n",
       "2  I am a big fan a Faerie Tale Theatre and I've ...          1\n",
       "3  I just finished reading a book about Dillinger...          0\n",
       "4  Greg Davis and Bryan Daly take some crazed sta...          0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df_train = load_imdb_reviews('./data/imdb_reviews_train_espanol.csv')\n",
    "df_train = load_imdb_reviews('./data/imdb_reviews_train.csv')\n",
    "print(\"\\nPrimeras 5 filas del conjunto de datos de entrenamiento de reseñas de IMDB en en español:\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "0    12500\n",
       "1    12500\n",
       "dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.groupby('sentiment').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimiendo 5 ejemplos de la clase negativa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If you ask me the first one was really better one. Look at Sarah M. G., she is real, mean, cruel girl, look at Amy Adams she is just little fool hanging around. She is nothing! People don\\'t adore her! Second, Sebastian was cute and hot in first movie, now he is \"baby face\". Story is not that good, and i do not understand. Why didn\\'t they make this one first, it is the beginning. Loosy actors, nothing with story. This is not cruel, this is playing. First one has better actors, better story, and its mean. I think that the music is better in cruel intentions 1 and the music is better in cruel intentions 3. It is not the worst movie I saw, but in compaer with first one its one big, big, big nothing.'\n",
      " 0]\n",
      "[\"I just finished reading a book about Dillinger. This movie was horribly inaccurate. It's like they got a list of names and just made everything up. His robberies and getaways were well planned, down to the second - when the time was up, they left whether they had all of the money or not. They had notes of every road, where to turn, etc. Purvis never saw him at the restaurant, he was told that Dillinger paid for his meal after Dillinger left. Purvis never even SAW Dillinger before the night Dillinger was killed, only photos of him. The way his gang members died were fictitious. Dillinger never robbed a bank by himself, like he did in this movie. If I had never read the book, maybe I could have enjoyed the movie. The acting was a bit over the top in places. The action was overdone as well. On second thought, I doubt if I would have enjoyed it much even if I HADN'T read the book.\"\n",
      " 0]\n",
      "['Greg Davis and Bryan Daly take some crazed statements by a terrorists, add some commentary by a bunch of uber-right reactionaries, ascribe the most extreme positions of the most fundamentalist Moslems on the planet to everyone who calls themselves a Moslem, and presents this as the theology of Islam. Maybe their next film will involve interviewing Fred Phelps and the congregation of the Westboro Baptist Church, adding commentary by some militant atheist \"scholars, and call their film \"What the World Needs to Know About Christianity.\" Ultimately, this film suffers from both poor production values and lack of attention to the most basic standards of journalism. Don\\'t waste your time and money; just turn on your AM radio and listen to Rush Limbaugh for a couple of days for free and you\\'ll get the same message with the same level of intellectual analysis.'\n",
      " 0]\n",
      "['As a writer I find films this bad making it into production a complete slap in the face. Talk about insulting. I was writing better stories than this in 8th grade. Bad acting, bad writing, bad directing and when added all together the result is complete and total failure. <br /><br />The only thing this movie manages to accomplish is tricking the unsuspecting consumer into wasting their time. Who would green light something so poorly written? It\\'s not artistic, clever, smart, suspenseful, mysterious, scary, dramatic-NOTHING.<br /><br />The characters are flat and boring with no development. The plot is as recycled as an aluminum can. They somehow managed to cast a few very familiar actors who all must be pretty desperate for work or hoping one of these low budget independent movies will turn out to be the next \"Pulp Fiction\". This script should have been used to line a bird cage, not a movie. <br /><br />Oh and last but not least, a 5\\'2 105 lb woman of course has the strength to kill men and women twice her size without a struggle and in a single blow. <br /><br />Avoid this bomb like it will infect you with an STD.'\n",
      " 0]\n",
      "[\"Not one of Keaton's best efforts, this was perhaps a veiled attempt to revenge himself on the family he married into - the Talmadges. A Polish/English language barrier and a series of coincidences leads Buster into a marriage with a large Irish woman, who (along with her father and brothers) treat him shabbily until they think he may be an heir to a fortune. Mistaken identities abound here - gags are set up and but for the main fail to pay off.<br /><br />This Metro short does have at least two real laughs - Buster's cleverly turning around his lack of dinner by using the calendar on the wall and the basic ignorance of his adopted family to literally bring the meat to his plate. The other is a family photo, with the entire group slowly collapsing to the floor as the tripod of the camera loses its stability.<br /><br />The yeast beer overflow could have been the catalyst for a massive series of gags built upon gags, but stops short (for all the buildup) of development.<br /><br />Kino's print is crisp and clear and the score is one for player piano, drums and sound effects. Not one of Buster's best efforts, but worth a few laughs.\"\n",
      " 0]\n"
     ]
    }
   ],
   "source": [
    "for review in np.array(df_train)[np.array(df_train.sentiment) == 0][:5]:\n",
    "    print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprimiendo 5 ejemplos de la clase positiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"In Panic In The Streets Richard Widmark plays U.S. Navy doctor who has his week rudely interrupted with a corpse that contains plague. As cop Paul Douglas properly points out the guy died from two bullets in the chest. That's not the issue here, the two of them become unwilling partners in an effort to find the killers and anyone else exposed to the disease.<br /><br />As was pointed out by any number of people, for some reason director Elia Kazan did not bother to cast the small parts with anyone that sounds like they're from Louisiana. Having been to New Orleans where the story takes place I can personally attest to that. Richard Widmark and his wife Barbara Bel Geddes can be excused because as a Navy doctor he could be assigned there, but for those that are natives it doesn't work.<br /><br />But with plague out there and the news being kept a secret, the New Orleans PD starts a dragnet of the city's underworld. The dead guy came off a ship from Europe and he had underworld connections. A New Orleans wise guy played by Jack Palance jumps to a whole bunch of erroneous conclusions and starts harassing a cousin of the dead guy who is starting to show plague symptoms. Palance got rave reviews in the first film where he received notice.<br /><br />Personally my favorite in this film is Zero Mostel. This happened right before Mostel was blacklisted and around that time he made a specialty of playing would be tough guys who are really toadies. He plays the same kind of role in the Humphrey Bogart film, The Enforcer. Sadly I can kind of identify with Mostel in that last chase scene where he and Palance are being chased down by Widmark, Douglas, and half the New Orleans Police. Seeing the weight challenged Zero trying to keep up with Palance was something else because I'm kind of in Zero's league now in the heft department.<br /><br />Kazan kept the action going at a good clip, there's very little down time in this film. If there was any less it would be an Indiana Jones film. Panic In The Streets won an Oscar for Best Original Screenplay that year.<br /><br />Kazan also made good use of the New Orleans waterfront and the French Quarter. Some of the same kinds of shots are later used in On the Waterfront. In fact Panic In The Streets is about people not squealing when they really should in their own best interest. Very similar again to On the Waterfront.<br /><br />Panic In The Streets does everyone proud who was associated with it. Now why couldn't Elia Kazan get some decent New Orleans sounding people in the small roles.\"\n",
      " 1]\n",
      "[\"I am a big fan a Faerie Tale Theatre and I've seen them all and this is one of the best! It's funny, romantic, and a classic. I recommend this for all ages. It's great for little kids because it's well, Cinderella and great for adults and teen because it's funny and not over the top. I watched it when I was little and I still watch it now. It has great lines that my family and I quote all the time. The acting is great and it never gets old. If you like fairy tales and romances you will love this. I've watched many a Cinderella movie in my time and this is the best of them all. (Sorry Disney) I highly recommend this movie and all the Faerie Tale Theatre shows. They all appeal to all ages and are all unique and very entertaining.\"\n",
      " 1]\n",
      "[\"This really is an incredible film. Not only does it document the eternal struggle of indigenous and disenfranchised people to gain their rightful voice but it also shows the United States up for its dishonesty, subterfuge, and blatant disregard for human rights and self-determination. Chavez is shown as a very brave and charismatic leader struggling against what can only be characterized as a despicable elite devoid of any sense of proportion or justice. These filmmakers have recorded a coup unlike anything witnessed before.<br /><br />And in the cross hairs we see the USA, once again pulling the strings and blurring all sense of reality. It's heart-breaking to watch the initial stages of the revolt knowing full well that the subversion of democracy that we're witnessing is a tool long used by successive American governments and their seemingly blinkered citizens. The footage makes it clear that this is not a manipulation of TV or generic footage but an active documentation of a people and its government fighting for its future. Truly a moving experience for anyone with a conscience. These Irish film makers deserve our gratitude. Long live Chavez.<br /><br />We need to enshrine the notion that each country must be allowed to choose its government and to develop in ways that the majority sees fit. First phase in this process is the need to know what the realities of the situation are, and this documentary does a great job of doing just that.\"\n",
      " 1]\n",
      "['If you lived through the 60s, this film can be at times painful and other times quite joyous. It\\'s all there but the small print in the counter culture tabloids prevalent at the time. These are the roots of a social revolution that is still playing out: \"don\\'t speak too soon for the wheel\\'s still in spin, for the times they are a-changin\\'\". While the film focuses on the revolutionary nature of LSD and it\\'s dissemination at the time, that alone played a tremendous hand in the evolution of the intelligentsia, influencing engineers, scientists and aiding in the hyper-development of computer related activities. A salute to the filmmakers from one who was there - you\\'ve captured the era better than I\\'ve seen before.'\n",
      " 1]\n",
      "['I\\'m 14 years old and I love this cartoon. Burt Reynolds and Dom Deluise make a great pair. This movie is really funny and I love the songs. My favorite songs are \"You can\\'t keep a good dog down\" and that song about sharing, I think it\\'s called \"What\\'s mine is yours\". This was the last movie with Judith Barsi, who played the voice of Anne-Marie. My favorite character is Charlie but I find Itchy\\'s voice is so fun to hear. Although some scenes I actually found scary, I still have a hard time watching the scene with Charlie\\'s dream, and Carface scares the crap out of me. Other characters like King Gator I found really funny. The ending was adorable and was actually sad, made me cry a little. I give this movie 7/10.'\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "for review in np.array(df_train)[np.array(df_train.sentiment) == 1][:5]:\n",
    "    print(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de texto\n",
    "\n",
    "Inicialmente crearemos una función para procesar y tokenizar el texto con las siguientes características:\n",
    "\n",
    "* Transformar el texto a minúsculas.\n",
    "* Eliminar caracteres que no sean letras. Para este problema de clasificación no parecen ser relevantes los caracteres puntuación, los números, ni caracteres especiales.\n",
    "* Solo conservamos tokens que contengan dos o más letras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original:\n",
      "In Panic In The Streets Richard Widmark plays U.S. Navy doctor who has his week rudely interrupted with a corpse that contains plague. As cop Paul Douglas properly points out the guy died from two bullets in the chest. That's not the issue here, the two of them become unwilling partners in an effort to find the killers and anyone else exposed to the disease.<br /><br />As was pointed out by any number of people, for some reason director Elia Kazan did not bother to cast the small parts with anyone that sounds like they're from Louisiana. Having been to New Orleans where the story takes place I can personally attest to that. Richard Widmark and his wife Barbara Bel Geddes can be excused because as a Navy doctor he could be assigned there, but for those that are natives it doesn't work.<br /><br />But with plague out there and the news being kept a secret, the New Orleans PD starts a dragnet of the city's underworld. The dead guy came off a ship from Europe and he had underworld connections. A New Orleans wise guy played by Jack Palance jumps to a whole bunch of erroneous conclusions and starts harassing a cousin of the dead guy who is starting to show plague symptoms. Palance got rave reviews in the first film where he received notice.<br /><br />Personally my favorite in this film is Zero Mostel. This happened right before Mostel was blacklisted and around that time he made a specialty of playing would be tough guys who are really toadies. He plays the same kind of role in the Humphrey Bogart film, The Enforcer. Sadly I can kind of identify with Mostel in that last chase scene where he and Palance are being chased down by Widmark, Douglas, and half the New Orleans Police. Seeing the weight challenged Zero trying to keep up with Palance was something else because I'm kind of in Zero's league now in the heft department.<br /><br />Kazan kept the action going at a good clip, there's very little down time in this film. If there was any less it would be an Indiana Jones film. Panic In The Streets won an Oscar for Best Original Screenplay that year.<br /><br />Kazan also made good use of the New Orleans waterfront and the French Quarter. Some of the same kinds of shots are later used in On the Waterfront. In fact Panic In The Streets is about people not squealing when they really should in their own best interest. Very similar again to On the Waterfront.<br /><br />Panic In The Streets does everyone proud who was associated with it. Now why couldn't Elia Kazan get some decent New Orleans sounding people in the small roles.\n",
      "Texto procesado y tokenizado:\n",
      "['in', 'panic', 'the', 'streets', 'richard', 'widmark', 'plays', 'navy', 'doctor', 'who', 'has', 'his', 'week', 'rudely', 'interrupted', 'with', 'corpse', 'that', 'contains', 'plague', 'cop', 'paul', 'douglas', 'properly', 'points', 'out', 'the', 'guy', 'died', 'from', 'two', 'bullets', 'the', 'chest', 'that', 'not', 'the', 'issue', 'here', 'the', 'two', 'them', 'become', 'unwilling', 'partners', 'effort', 'find', 'the', 'killers', 'and', 'anyone', 'else', 'exposed', 'the', 'disease', 'was', 'pointed', 'out', 'any', 'number', 'people', 'for', 'some', 'reason', 'director', 'elia', 'kazan', 'did', 'not', 'bother', 'cast', 'the', 'small', 'parts', 'with', 'anyone', 'that', 'sounds', 'like', 'they', 'from', 'louisiana', 'having', 'been', 'new', 'orleans', 'where', 'the', 'story', 'takes', 'place', 'can', 'personally', 'attest', 'that', 'richard', 'widmark', 'and', 'his', 'wife', 'barbara', 'bel', 'geddes', 'can', 'excused', 'because', 'navy', 'doctor', 'could', 'assigned', 'there', 'but', 'for', 'those', 'that', 'are', 'natives', 'doesn', 'work', 'but', 'with', 'plague', 'out', 'there', 'and', 'the', 'news', 'being', 'kept', 'secret', 'the', 'new', 'orleans', 'starts', 'dragnet', 'the', 'city', 'underworld', 'the', 'dead', 'guy', 'came', 'off', 'ship', 'from', 'europe', 'and', 'had', 'underworld', 'connections', 'new', 'orleans', 'wise', 'guy', 'played', 'jack', 'palance', 'jumps', 'whole', 'bunch', 'erroneous', 'conclusions', 'and', 'starts', 'harassing', 'cousin', 'the', 'dead', 'guy', 'who', 'starting', 'show', 'plague', 'symptoms', 'palance', 'got', 'rave', 'reviews', 'the', 'first', 'film', 'where', 'received', 'notice', 'personally', 'favorite', 'this', 'film', 'zero', 'mostel', 'this', 'happened', 'right', 'before', 'mostel', 'was', 'blacklisted', 'and', 'around', 'that', 'time', 'made', 'specialty', 'playing', 'would', 'tough', 'guys', 'who', 'are', 'really', 'toadies', 'plays', 'the', 'same', 'kind', 'role', 'the', 'humphrey', 'bogart', 'film', 'the', 'enforcer', 'sadly', 'can', 'kind', 'identify', 'with', 'mostel', 'that', 'last', 'chase', 'scene', 'where', 'and', 'palance', 'are', 'being', 'chased', 'down', 'widmark', 'douglas', 'and', 'half', 'the', 'new', 'orleans', 'police', 'seeing', 'the', 'weight', 'challenged', 'zero', 'trying', 'keep', 'with', 'palance', 'was', 'something', 'else', 'because', 'kind', 'zero', 'league', 'now', 'the', 'heft', 'department', 'kazan', 'kept', 'the', 'action', 'going', 'good', 'clip', 'there', 'very', 'little', 'down', 'time', 'this', 'film', 'there', 'was', 'any', 'less', 'would', 'indiana', 'jones', 'film', 'panic', 'the', 'streets', 'won', 'oscar', 'for', 'best', 'original', 'screenplay', 'that', 'year', 'kazan', 'also', 'made', 'good', 'use', 'the', 'new', 'orleans', 'waterfront', 'and', 'the', 'french', 'quarter', 'some', 'the', 'same', 'kinds', 'shots', 'are', 'later', 'used', 'the', 'waterfront', 'fact', 'panic', 'the', 'streets', 'about', 'people', 'not', 'squealing', 'when', 'they', 'really', 'should', 'their', 'own', 'best', 'interest', 'very', 'similar', 'again', 'the', 'waterfront', 'panic', 'the', 'streets', 'does', 'everyone', 'proud', 'who', 'was', 'associated', 'with', 'now', 'why', 'couldn', 'elia', 'kazan', 'get', 'some', 'decent', 'new', 'orleans', 'sounding', 'people', 'the', 'small', 'roles']\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text: str) -> \"list[str]\":\n",
    "    \"\"\"\n",
    "    Procesa un texto y luego lo tokeniza.\n",
    "\n",
    "    Args:\n",
    "        text(str) texto a procesar y tokenizar.\n",
    "\n",
    "    Returns:\n",
    "        list[str] lista de tokens.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^a-z]', ' ', text.lower()) # Convertir a minúsculas y eliminar caracteres no alfabéticos\n",
    "    text = re.sub(r'\\s[a-z]([a-z])?\\b', '', text) # Eliminar palabras de una o dos letras\n",
    "    return text.split() # Tokenizar\n",
    "\n",
    "\n",
    "sample = df_train.iloc[0].review\n",
    "print('Texto original:')\n",
    "print(sample)\n",
    "print('Texto procesado y tokenizado:')\n",
    "print(tokenize_text(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos constuir el vocabulario de nuestro corpus de entrenamiento. Este tamaño sería el tamaño que tendría la representación de bolsa de palabras para cada uno de los documentos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de tokens únicos: 72866\n"
     ]
    }
   ],
   "source": [
    "vocabulary = {}\n",
    "for r in np.array(df_train):\n",
    "    #print(r[0])\n",
    "    for token in tokenize_text(r[0]):\n",
    "        vocabulary[token] = vocabulary.get(token, 0) + 1\n",
    "\n",
    "print(f'Número de tokens únicos: {len(vocabulary)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver los tokens más frecuentes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 336758),\n",
       " ('and', 164143),\n",
       " ('this', 76007),\n",
       " ('that', 73287),\n",
       " ('was', 48209),\n",
       " ('for', 44345),\n",
       " ('with', 44130),\n",
       " ('movie', 44047),\n",
       " ('but', 42624),\n",
       " ('film', 40162)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ver los 10 tokens más frecuentes\n",
    "sorted(vocabulary.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estas tokens no parecen estar relacionados con el sentimiento de la reseña, por lo que podríamos eliminar los más frecuentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('toadies', 1),\n",
       " ('heft', 1),\n",
       " ('loosy', 1),\n",
       " ('compaer', 1),\n",
       " ('reactionaries', 1),\n",
       " ('enshrine', 1),\n",
       " ('changin', 1),\n",
       " ('acceptably', 1),\n",
       " ('talmadges', 1),\n",
       " ('yeast', 1)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(vocabulary.items(), key=lambda x: x[1])[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado los tokens menos frecuentes parecen ser nombres de películas, personajes, actores, entre otros, que tampoco parecen estar relacionados con la tarea de clasificación de sentimientos.\n",
    "\n",
    "# 2. Uso de Lexicones\n",
    "\n",
    "Adicionalmente a las representaciones de bolsa de palabras vistas en semanas anteriores. Podemos construir otro tipo de características que nosotros o expertos en el dominio de la tarea consideren relevantes para la solución del problema.\n",
    "\n",
    "En el caso de análisis de sentimiento existen diferentes lexicones en los que podemos encontrar la connotación negativa o positiva de las palabras. En esta sección utilizaremos el **Lexico Afinn del usuario de github jboscomendoza** que puede descargarse [aquí](https://raw.githubusercontent.com/jboscomendoza/lexicos-nrc-afinn/refs/heads/master/lexico_afinn.csv).\n",
    "\n",
    "Primero extraigamos los archivos correspondientes al lexicon descargado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lexicon = pd.read_csv('./data/lexico_afinn.csv')\n",
    "#lexicon.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type=weaksubj len=1 word1=abandoned pos1=adj stemmed1=n priorpolarity=negative\n",
      "\n",
      "type=weaksubj len=1 word1=abandonment pos1=noun stemmed1=n priorpolarity=negative\n",
      "\n",
      "type=weaksubj len=1 word1=abandon pos1=verb stemmed1=y priorpolarity=negative\n",
      "\n",
      "type=strongsubj len=1 word1=abase pos1=verb stemmed1=y priorpolarity=negative\n",
      "\n",
      "type=strongsubj len=1 word1=abasement pos1=anypos stemmed1=y priorpolarity=negative\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/subjclueslen1-HLTEMNLP05.tff') as f:\n",
    "    lexicon = f.readlines()\n",
    "\n",
    "for line in lexicon[:5]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según la documentación del lexicon, se resaltan las siguientes características:\n",
    "\n",
    "* palabra: Contiene la palabra en español.\n",
    "* puntuacion: Contiene la puntuación de la palabra en español que corresponde a la polaridad de la palabra, ya sea negativa, positiva o neutral, representada por numeros enteros que pueden ser positivos negativos o cero y cuyo valor indica mayor o menor subjetividad en el contexto.\n",
    "* word: Contiene la palabra en inglés.\n",
    "\n",
    "Uno de los grupos más simples de características que podemos extraer sobre el texto es la cantidad de palabras negativas y positivas que contiene. Así que primero podemos almacener de una manera conveniente los sets de palabras negativas y positivas contenidas en el lexicón."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#negative_words = set()\n",
    "#positive_words = set()\n",
    "\n",
    "# Iteramos sobre las filas del lexicon\n",
    "#for line in np.array(lexicon):\n",
    "    # Extraemos la palabra y la polaridad de la línea\n",
    "    #word = re.search(r'word1=(\\w+)', line).group(1)\n",
    "#    word = line[0]\n",
    "#    polarity = line[1]\n",
    "    # Guardamos la palabra en el conjunto correspondiente\n",
    "#    if polarity <= 0:\n",
    "#        negative_words.add(word)\n",
    "#    else:\n",
    "#print(\"Cantidad de palabras positivas:\", len(positive_words))\n",
    "#print(\"Cantidad de palabras negativas:\", len(negative_words))\n",
    "# Ver las primeras 10 palabras positivas\n",
    "#print(f'10 palabras positivas: {list(positive_words)[:10]}')\n",
    "# Ver las primeras 10 palabras negativas\n",
    "#print(f'10 palabras negativas: {list(negative_words)[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de palabras positivas: 2267\n",
      "Número de palabras negativas: 4103\n",
      "10 palabras positivas: ['flattering', 'intelligence', 'adroitly', 'frolic', 'lenient', 'angelic', 'shrewdly', 'stable', 'commonsensical', 'eager']\n",
      "10 palabras negativas: ['oversized', 'humbling', 'anti', 'fading', 'ejaculate', 'enjoin', 'sinisterly', 'distracting', 'dogmatic', 'mess']\n"
     ]
    }
   ],
   "source": [
    "negative_words = set()\n",
    "positive_words = set()\n",
    "\n",
    "# Iteramos sobre las líneas del lexicon\n",
    "for line in lexicon:\n",
    "    # Extraemos la palabra y la polaridad de la línea\n",
    "    word = re.search(r'word1=(\\w+)', line).group(1)\n",
    "    polarity = re.search(r'priorpolarity=(\\w+)', line).group(1)\n",
    "    # Guardamos la palabra en el conjunto correspondiente\n",
    "    if polarity == 'negative':\n",
    "        negative_words.add(word)\n",
    "    if polarity == 'positive':\n",
    "        positive_words.add(word)\n",
    "\n",
    "print(f'Número de palabras positivas: {len(positive_words)}')\n",
    "print(f'Número de palabras negativas: {len(negative_words)}')\n",
    "\n",
    "# Ver las primeras 10 palabras positivas\n",
    "print(f'10 palabras positivas: {list(positive_words)[:10]}')\n",
    "# Ver las primeras 10 palabras negativas\n",
    "print(f'10 palabras negativas: {list(negative_words)[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construyamos una clase similar a `CountVectorizer`de **scikit-learn** que nos permita extraer las características que definamos para cada documento del corpus.\n",
    "\n",
    "En este ejemplo vamos a extraer únicamente el número de palabras positivas y negativas contenidas en cada documento según el lexicón que estamos usando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representación vectorial de las primeras 5 reseñas:\n",
      "[[23 23]\n",
      " [17 16]\n",
      " [22  5]\n",
      " [10 11]\n",
      " [ 9  5]]\n"
     ]
    }
   ],
   "source": [
    "class LexiconVectorizer:\n",
    "    def __init__(self, positive_words: set, negative_words: set):\n",
    "        \"\"\"\n",
    "        Este vectorizador cuenta la cantidad de palabras positivas y negativas en un texto según los sets de palabras positivas y negativas con que se inicialice.\n",
    "\n",
    "        Args:\n",
    "            positive_words(set[str]): conjunto de palabras positivas.\n",
    "            negative_words(set[str]): conjunto de palabras negativas.\n",
    "        \"\"\"\n",
    "        self.positive_words = positive_words\n",
    "        self.negative_words = negative_words\n",
    "\n",
    "    # Esta vectorizador no utiliza los datos de entrenamiento, por lo que el método fit solo retorna self\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Este método transforma una lista de textos en una matriz de numpy donde cada fila es la cantidad de palabras positivas y negativas en el texto correspondiente.\n",
    "\n",
    "        Args:\n",
    "            X(list[str]): lista de textos a transformar.\n",
    "\n",
    "        Returns:\n",
    "            np.array: matriz de numpy con la cantidad de palabras positivas y negativas en cada texto.\n",
    "        \"\"\"\n",
    "        docs = []\n",
    "        for doc in X:\n",
    "            docs.append(self.count_words(doc))\n",
    "\n",
    "        return np.array(docs)\n",
    "                \n",
    "    def count_words(self, text: str) -> \"list[int]\":\n",
    "        \"\"\"\n",
    "        Este método cuenta la cantidad de palabras positivas y negativas en un texto.\n",
    "\n",
    "        Args:\n",
    "            text(str): texto a procesar.\n",
    "\n",
    "        Returns:\n",
    "            list[int]: lista con la cantidad de palabras positivas y negativas en el texto.\n",
    "        \"\"\"\n",
    "        proccessed_text = tokenize_text(text)\n",
    "        positive_count = sum([1 for word in proccessed_text if word in self.positive_words])\n",
    "        negative_count = sum([1 for word in proccessed_text if word in self.negative_words])\n",
    "\n",
    "        return [positive_count, negative_count]\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return \"LexiconVectorizer()\"\n",
    "    \n",
    "lexicon_vectorizer = LexiconVectorizer(positive_words, negative_words)\n",
    "X_train_lexicon = lexicon_vectorizer.fit_transform(df_train[\"review\"])\n",
    "print('Representación vectorial de las primeras 5 reseñas:')\n",
    "print(X_train_lexicon[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representación vectorial de la primera reseña:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 233 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "X_train_bow = bow_vectorizer.fit_transform(df_train[\"review\"])\n",
    "print('Representación vectorial de la primera reseña:')\n",
    "X_train_bow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La representación de CountVectorizer consiste en una matriz dispersa de 462 dimensiones, con 240 dimensiones con un valor mayor a 0.\n",
    "\n",
    "Nosotros también podemos crear una representación de texto personalizada en la que usemos la tradicional bolsa de palabras junto con las características extraídas de los lexicones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representación vectorial de la primera reseña:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1x74851 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 235 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class LexiconCountVectorizer(LexiconVectorizer):\n",
    "    def __init__(self, positive_words: set, negative_words: set, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Este vectorizador utiliza la representación generada por CountVectorizer junto con dos características sobre la proporción de palabras positivas y negativas identificadas en el documento.\n",
    "\n",
    "        Args:\n",
    "            positive_words(set[str]): conjunto de palabras positivas.\n",
    "            negative_words(set[str]): conjunto de palabras negativas.\n",
    "        \"\"\"\n",
    "        # Se inicializa la clase LexiconVectorizer\n",
    "        super().__init__(positive_words, negative_words)\n",
    "        # Se inicializa el CountVectorizer\n",
    "        self.count_vect = CountVectorizer(*args, **kwargs)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Este método ajusta el vectorizador a los datos de entrenamiento.\n",
    "\n",
    "        Args:\n",
    "            X(list[str]): lista de textos de entrenamiento.\n",
    "            y: no se utiliza.\n",
    "\n",
    "        Returns:\n",
    "            LexiconCountVectorizer: retorna el objeto actual.\n",
    "        \"\"\"\n",
    "        # Se ajusta el CountVectorizer a los datos de entrenamiento\n",
    "        self.count_vect.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Este método transforma una lista de textos en una matriz de numpy donde cada fila contiene las características extraídas de cada documento.\n",
    "\n",
    "        Args:\n",
    "            X(list[str]): lista de textos a transformar.\n",
    "\n",
    "        Returns:\n",
    "            np.array: matriz de numpy con la representación vectorial de cada documento.\n",
    "        \"\"\"\n",
    "        # Generar la representación de bolsa de palabras de los documentos\n",
    "        bow = self.count_vect.transform(X)\n",
    "        \n",
    "        # Generar las características de palabras positivas y negativas de los documentos\n",
    "        lexicon_features = []\n",
    "        for doc in X:\n",
    "            lexicon_features.append(self.count_words(doc))\n",
    "\n",
    "        # Se cálcula la proporción de palabras positivas y negativas en cada documento\n",
    "        lexicon_features = np.array(lexicon_features) / (np.sum(lexicon_features, axis=1).reshape(-1, 1) + 0.01) # Se suma 0.01 para evitar divisiones por cero\n",
    "        # Se convierte a una matriz dispersa del mismo tipo que la matriz de bolsa de palabras\n",
    "        lexicon_features = scipy.sparse.csr_matrix(lexicon_features)\n",
    "\n",
    "        # Se concatenan las características de bolsa de palabras y las características de palabras positivas y negativas\n",
    "        return scipy.sparse.hstack([lexicon_features, bow], format='csr')\n",
    "    \n",
    "    # set_params es un método utilizado por GridSearchCV para ajustar los parámetros del vectorizador\n",
    "    def set_params(self, **params):\n",
    "        \"\"\"\n",
    "        Este método ajusta los parámetros del CountVectorizer.\n",
    "\n",
    "        Args:\n",
    "            **params: parámetros a ajustar.\n",
    "\n",
    "        Returns:\n",
    "            LexiconCountVectorizer: retorna el objeto actual.\n",
    "        \"\"\"\n",
    "        self.count_vect.set_params(**params)\n",
    "        return self\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return \"LexiconCountVectorizer()\"\n",
    "    \n",
    "# Prueba del vectorizador\n",
    "bow_vectorizer = LexiconCountVectorizer(positive_words, negative_words)\n",
    "X_train_bow = bow_vectorizer.fit_transform(df_train[\"review\"])\n",
    "print('Representación vectorial de la primera reseña:')\n",
    "X_train_bow[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note como el resultado del vectorizador para la primera reseña tiene 2 dimensiones más que la representación de bolsa de palabras realizada en la celda anterior con `CountVectorizer`, y además la matriz dispersa contiene ahora 94 valores mayores a 0 en vez de 92.\n",
    "\n",
    "Ahora observe los valores contenidos en la representación vectorial del primer documento del corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t0.49989132797218\n",
      "  (0, 1)\t0.49989132797218\n",
      "  (0, 1279)\t1.0\n",
      "  (0, 1630)\t1.0\n",
      "  (0, 2150)\t1.0\n",
      "  (0, 2823)\t1.0\n",
      "  (0, 3169)\t3.0\n",
      "  (0, 3260)\t9.0\n",
      "  (0, 3705)\t2.0\n",
      "  (0, 3717)\t2.0\n",
      "  (0, 4126)\t4.0\n",
      "  (0, 4299)\t1.0\n",
      "  (0, 4467)\t3.0\n",
      "  (0, 4639)\t1.0\n",
      "  (0, 4660)\t1.0\n",
      "  (0, 4755)\t1.0\n",
      "  (0, 4894)\t1.0\n",
      "  (0, 5918)\t1.0\n",
      "  (0, 6336)\t4.0\n",
      "  (0, 6461)\t2.0\n",
      "  (0, 6484)\t1.0\n",
      "  (0, 6545)\t1.0\n",
      "  (0, 6581)\t1.0\n",
      "  (0, 6665)\t2.0\n",
      "  (0, 6684)\t1.0\n",
      "  :\t:\n",
      "  (0, 68771)\t2.0\n",
      "  (0, 69435)\t2.0\n",
      "  (0, 70308)\t1.0\n",
      "  (0, 70333)\t1.0\n",
      "  (0, 70507)\t1.0\n",
      "  (0, 70508)\t1.0\n",
      "  (0, 71161)\t2.0\n",
      "  (0, 72198)\t5.0\n",
      "  (0, 72283)\t3.0\n",
      "  (0, 72454)\t1.0\n",
      "  (0, 72499)\t1.0\n",
      "  (0, 72755)\t1.0\n",
      "  (0, 72759)\t3.0\n",
      "  (0, 72906)\t4.0\n",
      "  (0, 72913)\t1.0\n",
      "  (0, 72967)\t1.0\n",
      "  (0, 73006)\t3.0\n",
      "  (0, 73030)\t1.0\n",
      "  (0, 73289)\t1.0\n",
      "  (0, 73344)\t6.0\n",
      "  (0, 73493)\t1.0\n",
      "  (0, 73618)\t1.0\n",
      "  (0, 73716)\t2.0\n",
      "  (0, 74149)\t1.0\n",
      "  (0, 74611)\t3.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train_bow[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Entrenamiento de clasificadores de texto\n",
    "\n",
    "En esta sección usaremos los clasificadores de **regresión logística** y **naive bayes** para clasificar el sentimiento de nuestras reseñas de películas. Vamos a probar distintas representaciones de texto para encontrar el modelo que mejor se ajusta a nuestro problema. Para realizar estas pruebas de manera conveniente vamos a utilizar `GridSearch` que nos permite poner a prueba el desempeño de distintos pipelines, y distintos hiperparámetros. Adicionalmente `GridSearch` utiliza validación cruzada para medir el desempeño de los modelos de una manera más robusta.\n",
    "\n",
    "Consulte la siguiente guía para conocer otras técnicas de ajuste de hiperparámetros, y para entender mejor su funcionamiento. [Link a la guía](https://scikit-learn.org/stable/modules/grid_search.html#tips-for-parameter-search).\n",
    "\n",
    "**Advertencia**: La ejecución de esta celda puede tardar más de 30 minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "54 fits failed out of a total of 162.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_df' parameter of CountVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 0 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2091, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_df' parameter of TfidfVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 0 instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "18 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 469, in fit\n",
      "    Xt = self._fit(X, y, routed_params)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 406, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\joblib\\memory.py\", line 312, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 1310, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **params.get(\"fit_transform\", {}))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\aeveg\\AppData\\Local\\Temp\\ipykernel_18368\\2931114126.py\", line 50, in fit_transform\n",
      "  File \"C:\\Users\\aeveg\\AppData\\Local\\Temp\\ipykernel_18368\\3687669195.py\", line 27, in fit\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1323, in fit\n",
      "    self.fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'min_df' parameter of CountVectorizer must be a float in the range [0.0, 1.0] or an int in the range [1, inf). Got 0 instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\aeveg\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [       nan 0.85016031 0.84544023        nan 0.84936027 0.84480023\n",
      "        nan 0.84812026 0.84400025        nan 0.86168014 0.85864016\n",
      "        nan 0.86124013 0.85876016        nan 0.86100014 0.85856015\n",
      "        nan 0.85060028 0.84576023        nan 0.84972026 0.84528024\n",
      "        nan 0.84872027 0.84424023        nan 0.87559998 0.87251993\n",
      "        nan 0.87615996 0.87375993        nan 0.87635997 0.87423995\n",
      "        nan 0.88487998 0.88487998        nan 0.8857999  0.88567996\n",
      "        nan 0.88455993 0.88487995        nan 0.87627997 0.87351994\n",
      "        nan 0.87639998 0.87367992        nan 0.87651999 0.87451995]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor configuración:\n",
      "{'classifier': LogisticRegression(max_iter=1000), 'representation': TfidfVectorizer(), 'representation__max_df': 0.9, 'representation__min_df': 1}\n",
      "\n",
      "Mejor puntaje de validación cruzada: 0.89\n"
     ]
    }
   ],
   "source": [
    "# Definimos la secuencia de pasos del pipeline que en este caso consiste en un vectorizador y un clasificador\n",
    "pipeline = Pipeline([\n",
    "    ('representation', CountVectorizer()),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Definimos el espacio de búsqueda para cada paso del pipeline\n",
    "# Definimos el espacio de búsqueda como una lista de diccionarios si queremos probar diferentes hipérparámetros por configuración\n",
    "param_grid = [\n",
    "    # Probamos diferentes configuraciones  y adicionalmente probamos diferentes hipérparámetros para los vectorizadores\n",
    "    {\n",
    "    'representation': [CountVectorizer(), TfidfVectorizer(), LexiconCountVectorizer(positive_words, negative_words)],\n",
    "    'representation__max_df': [0.8, 0.9, 1.0],\n",
    "    'representation__min_df': [0, 1, 5],\n",
    "    'classifier': [MultinomialNB(), LogisticRegression(max_iter=1000)],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Realizamos la búsqueda de hiperparámetros\n",
    "search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=10)\n",
    "search.fit(df_train[\"review\"], df_train[\"sentiment\"])\n",
    "\n",
    "print(\"Mejor configuración:\\n{}\\n\".format(search.best_params_))\n",
    "print(\"Mejor puntaje de validación cruzada: {:.2f}\".format(search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La busqueda de hiperparámetros nos indica que la configuración que mejor resultados en promedio obtuvo fue la representación generada por `TfidfVectorizer`, en la que se utilizan todas las palabras del vocabulario ('representation__max_df': 1.0, y 'representation__min_df': 0) con un clasificador de regresión logística. En promedio esta configuración obtuvo un 89% de accuracy en las 5 particiones de evaluación generadas.\n",
    "\n",
    "Adicionalmente, la clase `GridSearchCV` posee el atributo `cv_results_` en donde podemos ver todos los resultados obtenidos para cada configuración probada. Este diccionario puede leerse fácilmente como un `DataFrame` de **pandas**, y darnos información importante como la duración del entrenamiento (en segundos), los parámetros probados en la configuración, y los puntajes obtenidos de entrenamiento y de evaluación por cada uno de las particiones realizadas de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_classifier</th>\n",
       "      <th>param_representation</th>\n",
       "      <th>param_representation__max_df</th>\n",
       "      <th>param_representation__min_df</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>12.899919</td>\n",
       "      <td>1.640529</td>\n",
       "      <td>5.622892</td>\n",
       "      <td>1.030805</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.888409</td>\n",
       "      <td>0.881675</td>\n",
       "      <td>0.887315</td>\n",
       "      <td>0.88580</td>\n",
       "      <td>0.002951</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>11.199655</td>\n",
       "      <td>1.830448</td>\n",
       "      <td>5.331626</td>\n",
       "      <td>1.046942</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.886729</td>\n",
       "      <td>0.882035</td>\n",
       "      <td>0.888276</td>\n",
       "      <td>0.88568</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>15.493603</td>\n",
       "      <td>3.405114</td>\n",
       "      <td>5.185460</td>\n",
       "      <td>0.241848</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.885289</td>\n",
       "      <td>0.881435</td>\n",
       "      <td>0.887916</td>\n",
       "      <td>0.88488</td>\n",
       "      <td>0.002661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>19.705483</td>\n",
       "      <td>1.341536</td>\n",
       "      <td>5.484385</td>\n",
       "      <td>0.834932</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.885289</td>\n",
       "      <td>0.881075</td>\n",
       "      <td>0.888276</td>\n",
       "      <td>0.88488</td>\n",
       "      <td>0.002954</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>10.994113</td>\n",
       "      <td>0.630157</td>\n",
       "      <td>4.948199</td>\n",
       "      <td>0.739835</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.886249</td>\n",
       "      <td>0.880355</td>\n",
       "      <td>0.888036</td>\n",
       "      <td>0.88488</td>\n",
       "      <td>0.003282</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>13.623768</td>\n",
       "      <td>0.990496</td>\n",
       "      <td>5.416306</td>\n",
       "      <td>1.117199</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.886369</td>\n",
       "      <td>0.879035</td>\n",
       "      <td>0.888276</td>\n",
       "      <td>0.88456</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>52.332077</td>\n",
       "      <td>6.017277</td>\n",
       "      <td>8.534399</td>\n",
       "      <td>1.236319</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.876770</td>\n",
       "      <td>0.874955</td>\n",
       "      <td>0.877835</td>\n",
       "      <td>0.87652</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>46.691273</td>\n",
       "      <td>5.659895</td>\n",
       "      <td>11.397270</td>\n",
       "      <td>1.581060</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.876890</td>\n",
       "      <td>0.875435</td>\n",
       "      <td>0.876875</td>\n",
       "      <td>0.87640</td>\n",
       "      <td>0.000682</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>36.064494</td>\n",
       "      <td>5.241977</td>\n",
       "      <td>6.311126</td>\n",
       "      <td>0.735945</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.877010</td>\n",
       "      <td>0.874955</td>\n",
       "      <td>0.877115</td>\n",
       "      <td>0.87636</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>38.382008</td>\n",
       "      <td>1.676876</td>\n",
       "      <td>10.605653</td>\n",
       "      <td>3.041964</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.877010</td>\n",
       "      <td>0.875195</td>\n",
       "      <td>0.876635</td>\n",
       "      <td>0.87628</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>27.749854</td>\n",
       "      <td>1.672022</td>\n",
       "      <td>7.032379</td>\n",
       "      <td>1.740419</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.877130</td>\n",
       "      <td>0.874595</td>\n",
       "      <td>0.876755</td>\n",
       "      <td>0.87616</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>21.562198</td>\n",
       "      <td>0.261821</td>\n",
       "      <td>5.990943</td>\n",
       "      <td>1.322278</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.876050</td>\n",
       "      <td>0.873395</td>\n",
       "      <td>0.877355</td>\n",
       "      <td>0.87560</td>\n",
       "      <td>0.001648</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>42.462303</td>\n",
       "      <td>1.363977</td>\n",
       "      <td>8.490078</td>\n",
       "      <td>1.321097</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.875690</td>\n",
       "      <td>0.871355</td>\n",
       "      <td>0.876515</td>\n",
       "      <td>0.87452</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>28.772546</td>\n",
       "      <td>0.976812</td>\n",
       "      <td>6.472956</td>\n",
       "      <td>2.889354</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.875570</td>\n",
       "      <td>0.870155</td>\n",
       "      <td>0.876995</td>\n",
       "      <td>0.87424</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>22.095930</td>\n",
       "      <td>2.137064</td>\n",
       "      <td>7.649670</td>\n",
       "      <td>1.706040</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.875570</td>\n",
       "      <td>0.870155</td>\n",
       "      <td>0.875555</td>\n",
       "      <td>0.87376</td>\n",
       "      <td>0.002549</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>38.716458</td>\n",
       "      <td>2.968422</td>\n",
       "      <td>10.022013</td>\n",
       "      <td>1.604965</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.875570</td>\n",
       "      <td>0.870635</td>\n",
       "      <td>0.874835</td>\n",
       "      <td>0.87368</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>37.231193</td>\n",
       "      <td>1.161626</td>\n",
       "      <td>10.748021</td>\n",
       "      <td>2.692065</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.875090</td>\n",
       "      <td>0.870755</td>\n",
       "      <td>0.874715</td>\n",
       "      <td>0.87352</td>\n",
       "      <td>0.001961</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>15.135372</td>\n",
       "      <td>0.521126</td>\n",
       "      <td>6.770788</td>\n",
       "      <td>1.684526</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>0.874250</td>\n",
       "      <td>0.870035</td>\n",
       "      <td>0.873275</td>\n",
       "      <td>0.87252</td>\n",
       "      <td>0.001802</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12.348883</td>\n",
       "      <td>0.359471</td>\n",
       "      <td>5.626869</td>\n",
       "      <td>0.718861</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.858171</td>\n",
       "      <td>0.859594</td>\n",
       "      <td>0.867275</td>\n",
       "      <td>0.86168</td>\n",
       "      <td>0.003998</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12.649597</td>\n",
       "      <td>0.720105</td>\n",
       "      <td>5.449409</td>\n",
       "      <td>1.291158</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.857931</td>\n",
       "      <td>0.858994</td>\n",
       "      <td>0.866795</td>\n",
       "      <td>0.86124</td>\n",
       "      <td>0.003952</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>11.493647</td>\n",
       "      <td>2.043474</td>\n",
       "      <td>5.076569</td>\n",
       "      <td>0.468129</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.857451</td>\n",
       "      <td>0.858634</td>\n",
       "      <td>0.866915</td>\n",
       "      <td>0.86100</td>\n",
       "      <td>0.004210</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>11.299102</td>\n",
       "      <td>0.231678</td>\n",
       "      <td>5.893038</td>\n",
       "      <td>1.322238</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.854812</td>\n",
       "      <td>0.856114</td>\n",
       "      <td>0.865355</td>\n",
       "      <td>0.85876</td>\n",
       "      <td>0.004693</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10.711932</td>\n",
       "      <td>0.582295</td>\n",
       "      <td>5.977757</td>\n",
       "      <td>1.100392</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.854572</td>\n",
       "      <td>0.855874</td>\n",
       "      <td>0.865475</td>\n",
       "      <td>0.85864</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>11.189907</td>\n",
       "      <td>2.537110</td>\n",
       "      <td>5.262169</td>\n",
       "      <td>1.427965</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.854812</td>\n",
       "      <td>0.855394</td>\n",
       "      <td>0.865475</td>\n",
       "      <td>0.85856</td>\n",
       "      <td>0.004895</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>33.111870</td>\n",
       "      <td>1.209890</td>\n",
       "      <td>9.625552</td>\n",
       "      <td>1.348615</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.843653</td>\n",
       "      <td>0.852514</td>\n",
       "      <td>0.855634</td>\n",
       "      <td>0.85060</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11.154424</td>\n",
       "      <td>1.561729</td>\n",
       "      <td>5.573206</td>\n",
       "      <td>1.331718</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.842453</td>\n",
       "      <td>0.852274</td>\n",
       "      <td>0.855754</td>\n",
       "      <td>0.85016</td>\n",
       "      <td>0.005632</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>33.756213</td>\n",
       "      <td>0.702935</td>\n",
       "      <td>10.547350</td>\n",
       "      <td>1.139788</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.843293</td>\n",
       "      <td>0.851314</td>\n",
       "      <td>0.854554</td>\n",
       "      <td>0.84972</td>\n",
       "      <td>0.004734</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.185674</td>\n",
       "      <td>1.662659</td>\n",
       "      <td>5.438824</td>\n",
       "      <td>1.256867</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.842573</td>\n",
       "      <td>0.851194</td>\n",
       "      <td>0.854314</td>\n",
       "      <td>0.84936</td>\n",
       "      <td>0.004966</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>34.617470</td>\n",
       "      <td>2.380332</td>\n",
       "      <td>9.523776</td>\n",
       "      <td>1.756018</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.842093</td>\n",
       "      <td>0.850834</td>\n",
       "      <td>0.853234</td>\n",
       "      <td>0.84872</td>\n",
       "      <td>0.004788</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.672321</td>\n",
       "      <td>1.011833</td>\n",
       "      <td>6.062320</td>\n",
       "      <td>1.724133</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.841733</td>\n",
       "      <td>0.849994</td>\n",
       "      <td>0.852634</td>\n",
       "      <td>0.84812</td>\n",
       "      <td>0.004644</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>34.071780</td>\n",
       "      <td>1.952875</td>\n",
       "      <td>10.395390</td>\n",
       "      <td>0.910949</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.839933</td>\n",
       "      <td>0.845914</td>\n",
       "      <td>0.851434</td>\n",
       "      <td>0.84576</td>\n",
       "      <td>0.004697</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.258451</td>\n",
       "      <td>1.746821</td>\n",
       "      <td>6.980870</td>\n",
       "      <td>1.705020</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.839573</td>\n",
       "      <td>0.845674</td>\n",
       "      <td>0.851074</td>\n",
       "      <td>0.84544</td>\n",
       "      <td>0.004698</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>30.370953</td>\n",
       "      <td>2.543312</td>\n",
       "      <td>9.280132</td>\n",
       "      <td>1.781747</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.839213</td>\n",
       "      <td>0.846514</td>\n",
       "      <td>0.850114</td>\n",
       "      <td>0.84528</td>\n",
       "      <td>0.004535</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12.351847</td>\n",
       "      <td>0.720372</td>\n",
       "      <td>6.670366</td>\n",
       "      <td>0.875564</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.838973</td>\n",
       "      <td>0.845434</td>\n",
       "      <td>0.849994</td>\n",
       "      <td>0.84480</td>\n",
       "      <td>0.004522</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>28.831861</td>\n",
       "      <td>0.260439</td>\n",
       "      <td>9.411206</td>\n",
       "      <td>2.034827</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.838373</td>\n",
       "      <td>0.844354</td>\n",
       "      <td>0.849994</td>\n",
       "      <td>0.84424</td>\n",
       "      <td>0.004745</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>14.504766</td>\n",
       "      <td>1.105729</td>\n",
       "      <td>4.418858</td>\n",
       "      <td>0.402458</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>0.837653</td>\n",
       "      <td>0.844234</td>\n",
       "      <td>0.850114</td>\n",
       "      <td>0.84400</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.060712</td>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.022720</td>\n",
       "      <td>0.007322</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.016477</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.012181</td>\n",
       "      <td>0.003712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.015795</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.015893</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.018135</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.019349</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.014162</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': MultinomialNB(), 'representatio...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.015895</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.017064</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.012691</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.031887</td>\n",
       "      <td>0.013771</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.014146</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.028474</td>\n",
       "      <td>0.008719</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.016080</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.019667</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.018008</td>\n",
       "      <td>0.004768</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>LogisticRegression(max_iter=1000)</td>\n",
       "      <td>LexiconCountVectorizer()</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>{'classifier': LogisticRegression(max_iter=100...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "40      12.899919      1.640529         5.622892        1.030805   \n",
       "41      11.199655      1.830448         5.331626        1.046942   \n",
       "38      15.493603      3.405114         5.185460        0.241848   \n",
       "37      19.705483      1.341536         5.484385        0.834932   \n",
       "44      10.994113      0.630157         4.948199        0.739835   \n",
       "43      13.623768      0.990496         5.416306        1.117199   \n",
       "52      52.332077      6.017277         8.534399        1.236319   \n",
       "49      46.691273      5.659895        11.397270        1.581060   \n",
       "34      36.064494      5.241977         6.311126        0.735945   \n",
       "46      38.382008      1.676876        10.605653        3.041964   \n",
       "31      27.749854      1.672022         7.032379        1.740419   \n",
       "28      21.562198      0.261821         5.990943        1.322278   \n",
       "53      42.462303      1.363977         8.490078        1.321097   \n",
       "35      28.772546      0.976812         6.472956        2.889354   \n",
       "32      22.095930      2.137064         7.649670        1.706040   \n",
       "50      38.716458      2.968422        10.022013        1.604965   \n",
       "47      37.231193      1.161626        10.748021        2.692065   \n",
       "29      15.135372      0.521126         6.770788        1.684526   \n",
       "10      12.348883      0.359471         5.626869        0.718861   \n",
       "13      12.649597      0.720105         5.449409        1.291158   \n",
       "16      11.493647      2.043474         5.076569        0.468129   \n",
       "14      11.299102      0.231678         5.893038        1.322238   \n",
       "11      10.711932      0.582295         5.977757        1.100392   \n",
       "17      11.189907      2.537110         5.262169        1.427965   \n",
       "19      33.111870      1.209890         9.625552        1.348615   \n",
       "1       11.154424      1.561729         5.573206        1.331718   \n",
       "22      33.756213      0.702935        10.547350        1.139788   \n",
       "4       13.185674      1.662659         5.438824        1.256867   \n",
       "25      34.617470      2.380332         9.523776        1.756018   \n",
       "7       12.672321      1.011833         6.062320        1.724133   \n",
       "20      34.071780      1.952875        10.395390        0.910949   \n",
       "2       11.258451      1.746821         6.980870        1.705020   \n",
       "23      30.370953      2.543312         9.280132        1.781747   \n",
       "5       12.351847      0.720372         6.670366        0.875564   \n",
       "26      28.831861      0.260439         9.411206        2.034827   \n",
       "8       14.504766      1.105729         4.418858        0.402458   \n",
       "0        0.060712      0.008984         0.000000        0.000000   \n",
       "3        0.022720      0.007322         0.000000        0.000000   \n",
       "6        0.016477      0.000410         0.000000        0.000000   \n",
       "9        0.012181      0.003712         0.000000        0.000000   \n",
       "12       0.015795      0.001429         0.000000        0.000000   \n",
       "15       0.015893      0.001670         0.000000        0.000000   \n",
       "18       0.018135      0.001733         0.000000        0.000000   \n",
       "21       0.019349      0.004950         0.000000        0.000000   \n",
       "24       0.014162      0.002321         0.000000        0.000000   \n",
       "27       0.015895      0.001952         0.000000        0.000000   \n",
       "30       0.017064      0.003694         0.000000        0.000000   \n",
       "33       0.012691      0.002588         0.000000        0.000000   \n",
       "36       0.031887      0.013771         0.000000        0.000000   \n",
       "39       0.014146      0.001700         0.000000        0.000000   \n",
       "42       0.028474      0.008719         0.000000        0.000000   \n",
       "45       0.016080      0.003511         0.000000        0.000000   \n",
       "48       0.019667      0.001602         0.000000        0.000000   \n",
       "51       0.018008      0.004768         0.000000        0.000000   \n",
       "\n",
       "                     param_classifier      param_representation  \\\n",
       "40  LogisticRegression(max_iter=1000)         TfidfVectorizer()   \n",
       "41  LogisticRegression(max_iter=1000)         TfidfVectorizer()   \n",
       "38  LogisticRegression(max_iter=1000)         TfidfVectorizer()   \n",
       "37  LogisticRegression(max_iter=1000)         TfidfVectorizer()   \n",
       "44  LogisticRegression(max_iter=1000)         TfidfVectorizer()   \n",
       "43  LogisticRegression(max_iter=1000)         TfidfVectorizer()   \n",
       "52  LogisticRegression(max_iter=1000)  LexiconCountVectorizer()   \n",
       "49  LogisticRegression(max_iter=1000)  LexiconCountVectorizer()   \n",
       "34  LogisticRegression(max_iter=1000)         CountVectorizer()   \n",
       "46  LogisticRegression(max_iter=1000)  LexiconCountVectorizer()   \n",
       "31  LogisticRegression(max_iter=1000)         CountVectorizer()   \n",
       "28  LogisticRegression(max_iter=1000)         CountVectorizer()   \n",
       "53  LogisticRegression(max_iter=1000)  LexiconCountVectorizer()   \n",
       "35  LogisticRegression(max_iter=1000)         CountVectorizer()   \n",
       "32  LogisticRegression(max_iter=1000)         CountVectorizer()   \n",
       "50  LogisticRegression(max_iter=1000)  LexiconCountVectorizer()   \n",
       "47  LogisticRegression(max_iter=1000)  LexiconCountVectorizer()   \n",
       "29  LogisticRegression(max_iter=1000)         CountVectorizer()   \n",
       "10                    MultinomialNB()         TfidfVectorizer()   \n",
       "13                    MultinomialNB()         TfidfVectorizer()   \n",
       "16                    MultinomialNB()         TfidfVectorizer()   \n",
       "14                    MultinomialNB()         TfidfVectorizer()   \n",
       "11                    MultinomialNB()         TfidfVectorizer()   \n",
       "17                    MultinomialNB()         TfidfVectorizer()   \n",
       "19                    MultinomialNB()  LexiconCountVectorizer()   \n",
       "1                     MultinomialNB()         CountVectorizer()   \n",
       "22                    MultinomialNB()  LexiconCountVectorizer()   \n",
       "4                     MultinomialNB()         CountVectorizer()   \n",
       "25                    MultinomialNB()  LexiconCountVectorizer()   \n",
       "7                     MultinomialNB()         CountVectorizer()   \n",
       "20                    MultinomialNB()  LexiconCountVectorizer()   \n",
       "2                     MultinomialNB()         CountVectorizer()   \n",
       "23                    MultinomialNB()  LexiconCountVectorizer()   \n",
       "5                     MultinomialNB()         CountVectorizer()   \n",
       "26                    MultinomialNB()  LexiconCountVectorizer()   \n",
       "8                     MultinomialNB()         CountVectorizer()   \n",
       "0                     MultinomialNB()         CountVectorizer()   \n",
       "3                     MultinomialNB()         CountVectorizer()   \n",
       "6                     MultinomialNB()         CountVectorizer()   \n",
       "9                     MultinomialNB()         TfidfVectorizer()   \n",
       "12                    MultinomialNB()         TfidfVectorizer()   \n",
       "15                    MultinomialNB()         TfidfVectorizer()   \n",
       "18                    MultinomialNB()  LexiconCountVectorizer()   \n",
       "21                    MultinomialNB()  LexiconCountVectorizer()   \n",
       "24                    MultinomialNB()  LexiconCountVectorizer()   \n",
       "27  LogisticRegression(max_iter=1000)         CountVectorizer()   \n",
       "30  LogisticRegression(max_iter=1000)         CountVectorizer()   \n",
       "33  LogisticRegression(max_iter=1000)         CountVectorizer()   \n",
       "36  LogisticRegression(max_iter=1000)         TfidfVectorizer()   \n",
       "39  LogisticRegression(max_iter=1000)         TfidfVectorizer()   \n",
       "42  LogisticRegression(max_iter=1000)         TfidfVectorizer()   \n",
       "45  LogisticRegression(max_iter=1000)  LexiconCountVectorizer()   \n",
       "48  LogisticRegression(max_iter=1000)  LexiconCountVectorizer()   \n",
       "51  LogisticRegression(max_iter=1000)  LexiconCountVectorizer()   \n",
       "\n",
       "    param_representation__max_df  param_representation__min_df  \\\n",
       "40                           0.9                             1   \n",
       "41                           0.9                             5   \n",
       "38                           0.8                             5   \n",
       "37                           0.8                             1   \n",
       "44                           1.0                             5   \n",
       "43                           1.0                             1   \n",
       "52                           1.0                             1   \n",
       "49                           0.9                             1   \n",
       "34                           1.0                             1   \n",
       "46                           0.8                             1   \n",
       "31                           0.9                             1   \n",
       "28                           0.8                             1   \n",
       "53                           1.0                             5   \n",
       "35                           1.0                             5   \n",
       "32                           0.9                             5   \n",
       "50                           0.9                             5   \n",
       "47                           0.8                             5   \n",
       "29                           0.8                             5   \n",
       "10                           0.8                             1   \n",
       "13                           0.9                             1   \n",
       "16                           1.0                             1   \n",
       "14                           0.9                             5   \n",
       "11                           0.8                             5   \n",
       "17                           1.0                             5   \n",
       "19                           0.8                             1   \n",
       "1                            0.8                             1   \n",
       "22                           0.9                             1   \n",
       "4                            0.9                             1   \n",
       "25                           1.0                             1   \n",
       "7                            1.0                             1   \n",
       "20                           0.8                             5   \n",
       "2                            0.8                             5   \n",
       "23                           0.9                             5   \n",
       "5                            0.9                             5   \n",
       "26                           1.0                             5   \n",
       "8                            1.0                             5   \n",
       "0                            0.8                             0   \n",
       "3                            0.9                             0   \n",
       "6                            1.0                             0   \n",
       "9                            0.8                             0   \n",
       "12                           0.9                             0   \n",
       "15                           1.0                             0   \n",
       "18                           0.8                             0   \n",
       "21                           0.9                             0   \n",
       "24                           1.0                             0   \n",
       "27                           0.8                             0   \n",
       "30                           0.9                             0   \n",
       "33                           1.0                             0   \n",
       "36                           0.8                             0   \n",
       "39                           0.9                             0   \n",
       "42                           1.0                             0   \n",
       "45                           0.8                             0   \n",
       "48                           0.9                             0   \n",
       "51                           1.0                             0   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "40  {'classifier': LogisticRegression(max_iter=100...           0.888409   \n",
       "41  {'classifier': LogisticRegression(max_iter=100...           0.886729   \n",
       "38  {'classifier': LogisticRegression(max_iter=100...           0.885289   \n",
       "37  {'classifier': LogisticRegression(max_iter=100...           0.885289   \n",
       "44  {'classifier': LogisticRegression(max_iter=100...           0.886249   \n",
       "43  {'classifier': LogisticRegression(max_iter=100...           0.886369   \n",
       "52  {'classifier': LogisticRegression(max_iter=100...           0.876770   \n",
       "49  {'classifier': LogisticRegression(max_iter=100...           0.876890   \n",
       "34  {'classifier': LogisticRegression(max_iter=100...           0.877010   \n",
       "46  {'classifier': LogisticRegression(max_iter=100...           0.877010   \n",
       "31  {'classifier': LogisticRegression(max_iter=100...           0.877130   \n",
       "28  {'classifier': LogisticRegression(max_iter=100...           0.876050   \n",
       "53  {'classifier': LogisticRegression(max_iter=100...           0.875690   \n",
       "35  {'classifier': LogisticRegression(max_iter=100...           0.875570   \n",
       "32  {'classifier': LogisticRegression(max_iter=100...           0.875570   \n",
       "50  {'classifier': LogisticRegression(max_iter=100...           0.875570   \n",
       "47  {'classifier': LogisticRegression(max_iter=100...           0.875090   \n",
       "29  {'classifier': LogisticRegression(max_iter=100...           0.874250   \n",
       "10  {'classifier': MultinomialNB(), 'representatio...           0.858171   \n",
       "13  {'classifier': MultinomialNB(), 'representatio...           0.857931   \n",
       "16  {'classifier': MultinomialNB(), 'representatio...           0.857451   \n",
       "14  {'classifier': MultinomialNB(), 'representatio...           0.854812   \n",
       "11  {'classifier': MultinomialNB(), 'representatio...           0.854572   \n",
       "17  {'classifier': MultinomialNB(), 'representatio...           0.854812   \n",
       "19  {'classifier': MultinomialNB(), 'representatio...           0.843653   \n",
       "1   {'classifier': MultinomialNB(), 'representatio...           0.842453   \n",
       "22  {'classifier': MultinomialNB(), 'representatio...           0.843293   \n",
       "4   {'classifier': MultinomialNB(), 'representatio...           0.842573   \n",
       "25  {'classifier': MultinomialNB(), 'representatio...           0.842093   \n",
       "7   {'classifier': MultinomialNB(), 'representatio...           0.841733   \n",
       "20  {'classifier': MultinomialNB(), 'representatio...           0.839933   \n",
       "2   {'classifier': MultinomialNB(), 'representatio...           0.839573   \n",
       "23  {'classifier': MultinomialNB(), 'representatio...           0.839213   \n",
       "5   {'classifier': MultinomialNB(), 'representatio...           0.838973   \n",
       "26  {'classifier': MultinomialNB(), 'representatio...           0.838373   \n",
       "8   {'classifier': MultinomialNB(), 'representatio...           0.837653   \n",
       "0   {'classifier': MultinomialNB(), 'representatio...                NaN   \n",
       "3   {'classifier': MultinomialNB(), 'representatio...                NaN   \n",
       "6   {'classifier': MultinomialNB(), 'representatio...                NaN   \n",
       "9   {'classifier': MultinomialNB(), 'representatio...                NaN   \n",
       "12  {'classifier': MultinomialNB(), 'representatio...                NaN   \n",
       "15  {'classifier': MultinomialNB(), 'representatio...                NaN   \n",
       "18  {'classifier': MultinomialNB(), 'representatio...                NaN   \n",
       "21  {'classifier': MultinomialNB(), 'representatio...                NaN   \n",
       "24  {'classifier': MultinomialNB(), 'representatio...                NaN   \n",
       "27  {'classifier': LogisticRegression(max_iter=100...                NaN   \n",
       "30  {'classifier': LogisticRegression(max_iter=100...                NaN   \n",
       "33  {'classifier': LogisticRegression(max_iter=100...                NaN   \n",
       "36  {'classifier': LogisticRegression(max_iter=100...                NaN   \n",
       "39  {'classifier': LogisticRegression(max_iter=100...                NaN   \n",
       "42  {'classifier': LogisticRegression(max_iter=100...                NaN   \n",
       "45  {'classifier': LogisticRegression(max_iter=100...                NaN   \n",
       "48  {'classifier': LogisticRegression(max_iter=100...                NaN   \n",
       "51  {'classifier': LogisticRegression(max_iter=100...                NaN   \n",
       "\n",
       "    split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "40           0.881675           0.887315          0.88580        0.002951   \n",
       "41           0.882035           0.888276          0.88568        0.002653   \n",
       "38           0.881435           0.887916          0.88488        0.002661   \n",
       "37           0.881075           0.888276          0.88488        0.002954   \n",
       "44           0.880355           0.888036          0.88488        0.003282   \n",
       "43           0.879035           0.888276          0.88456        0.003983   \n",
       "52           0.874955           0.877835          0.87652        0.001189   \n",
       "49           0.875435           0.876875          0.87640        0.000682   \n",
       "34           0.874955           0.877115          0.87636        0.000994   \n",
       "46           0.875195           0.876635          0.87628        0.000782   \n",
       "31           0.874595           0.876755          0.87616        0.001117   \n",
       "28           0.873395           0.877355          0.87560        0.001648   \n",
       "53           0.871355           0.876515          0.87452        0.002263   \n",
       "35           0.870155           0.876995          0.87424        0.002947   \n",
       "32           0.870155           0.875555          0.87376        0.002549   \n",
       "50           0.870635           0.874835          0.87368        0.002174   \n",
       "47           0.870755           0.874715          0.87352        0.001961   \n",
       "29           0.870035           0.873275          0.87252        0.001802   \n",
       "10           0.859594           0.867275          0.86168        0.003998   \n",
       "13           0.858994           0.866795          0.86124        0.003952   \n",
       "16           0.858634           0.866915          0.86100        0.004210   \n",
       "14           0.856114           0.865355          0.85876        0.004693   \n",
       "11           0.855874           0.865475          0.85864        0.004862   \n",
       "17           0.855394           0.865475          0.85856        0.004895   \n",
       "19           0.852514           0.855634          0.85060        0.005075   \n",
       "1            0.852274           0.855754          0.85016        0.005632   \n",
       "22           0.851314           0.854554          0.84972        0.004734   \n",
       "4            0.851194           0.854314          0.84936        0.004966   \n",
       "25           0.850834           0.853234          0.84872        0.004788   \n",
       "7            0.849994           0.852634          0.84812        0.004644   \n",
       "20           0.845914           0.851434          0.84576        0.004697   \n",
       "2            0.845674           0.851074          0.84544        0.004698   \n",
       "23           0.846514           0.850114          0.84528        0.004535   \n",
       "5            0.845434           0.849994          0.84480        0.004522   \n",
       "26           0.844354           0.849994          0.84424        0.004745   \n",
       "8            0.844234           0.850114          0.84400        0.005090   \n",
       "0                 NaN                NaN              NaN             NaN   \n",
       "3                 NaN                NaN              NaN             NaN   \n",
       "6                 NaN                NaN              NaN             NaN   \n",
       "9                 NaN                NaN              NaN             NaN   \n",
       "12                NaN                NaN              NaN             NaN   \n",
       "15                NaN                NaN              NaN             NaN   \n",
       "18                NaN                NaN              NaN             NaN   \n",
       "21                NaN                NaN              NaN             NaN   \n",
       "24                NaN                NaN              NaN             NaN   \n",
       "27                NaN                NaN              NaN             NaN   \n",
       "30                NaN                NaN              NaN             NaN   \n",
       "33                NaN                NaN              NaN             NaN   \n",
       "36                NaN                NaN              NaN             NaN   \n",
       "39                NaN                NaN              NaN             NaN   \n",
       "42                NaN                NaN              NaN             NaN   \n",
       "45                NaN                NaN              NaN             NaN   \n",
       "48                NaN                NaN              NaN             NaN   \n",
       "51                NaN                NaN              NaN             NaN   \n",
       "\n",
       "    rank_test_score  \n",
       "40                1  \n",
       "41                2  \n",
       "38                3  \n",
       "37                3  \n",
       "44                5  \n",
       "43                6  \n",
       "52                7  \n",
       "49                8  \n",
       "34                9  \n",
       "46               10  \n",
       "31               11  \n",
       "28               12  \n",
       "53               13  \n",
       "35               14  \n",
       "32               15  \n",
       "50               16  \n",
       "47               17  \n",
       "29               18  \n",
       "10               19  \n",
       "13               20  \n",
       "16               21  \n",
       "14               22  \n",
       "11               23  \n",
       "17               24  \n",
       "19               25  \n",
       "1                26  \n",
       "22               27  \n",
       "4                28  \n",
       "25               29  \n",
       "7                30  \n",
       "20               31  \n",
       "2                32  \n",
       "23               33  \n",
       "5                34  \n",
       "26               35  \n",
       "8                36  \n",
       "0                37  \n",
       "3                37  \n",
       "6                37  \n",
       "9                37  \n",
       "12               37  \n",
       "15               37  \n",
       "18               37  \n",
       "21               37  \n",
       "24               37  \n",
       "27               37  \n",
       "30               37  \n",
       "33               37  \n",
       "36               37  \n",
       "39               37  \n",
       "42               37  \n",
       "45               37  \n",
       "48               37  \n",
       "51               37  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mostramos los resultados de la búsqueda de hiperparámetros ordenados por el puntaje de validación cruzada promedio de mayor a menor\n",
    "df_results = pd.DataFrame(search.cv_results_)\n",
    "df_results.sort_values(by='mean_test_score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la tabla anterior se evidencia que en general los modelos de Regresión Logística obtienen un mejor desemepeño que los modelos de Naive Bayes. También se observa que en general las mejores representaciones de texto son `TfidfVectorizer`, `LexiconCountVectorizer`, `CountVectorizer`, y `LexiconVectorizer` en el orden correspondiente, y finalmente se observa que para los vectorizadores los mejores hiperpárametros son utilizar todas las palabras presentes en el corpus a excepción de aquellas con una única ocurrencia.\n",
    "\n",
    "# 4. Evaluación de los clasificadores\n",
    "\n",
    "Finalmente, hacemos uso del set de evaluación para validar el desempeño de nuestro modelo.\n",
    "\n",
    "En primer lugar, cargamos el set de evaluación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Primeras 5 filas del conjunto de datos de prueba de reseñas de IMDB en en español:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When I was a kid, I loved \"Tiny Toons\". I espe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The setup for \"Nature of the Beast\" is ingenio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I do not have much to say than this is a great...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Extremely formulaic with cosmic-sized logic ho...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I actually liked certain things about this gam...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  When I was a kid, I loved \"Tiny Toons\". I espe...          1\n",
       "1  The setup for \"Nature of the Beast\" is ingenio...          0\n",
       "2  I do not have much to say than this is a great...          1\n",
       "3  Extremely formulaic with cosmic-sized logic ho...          0\n",
       "4  I actually liked certain things about this gam...          0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = load_imdb_reviews('./data/imdb_reviews_test.csv')\n",
    "print(\"\\nPrimeras 5 filas del conjunto de datos de prueba de reseñas de IMDB en en español:\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora calculamos el accuracy del modelo con mejor desempeño:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8854"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_estimator_.score(df_test[\"review\"], df_test[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos ver la matriz de confusión generada por nuestro modelo en el set evaluación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Realizamos la predicción sobre el conjunto de prueba\n",
    "y_pred = search.best_estimator_.predict(df_test[\"review\"])\n",
    "\n",
    "# Construimos la matriz de confusión\n",
    "conf_mtx = confusion_matrix(df_test[\"sentiment\"], y_pred, normalize='true')\n",
    "\n",
    "# Mostramos la matriz de confusión\n",
    "disp = ConfusionMatrixDisplay(conf_mtx, display_labels=[\"neg\",\"pos\"])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras métricas que pueder ser relevantes según el problema son el `f1-score`, la precisión, y la sensitividad, ya sean macro o micro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.88      0.89      0.89     12500\n",
      "         pos       0.89      0.88      0.89     12500\n",
      "\n",
      "    accuracy                           0.89     25000\n",
      "   macro avg       0.89      0.89      0.89     25000\n",
      "weighted avg       0.89      0.89      0.89     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(df_test[\"sentiment\"], y_pred, target_names=[\"neg\", \"pos\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Veamos ejemplos de clasificación de nuestro modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ejemplo positivo: 1\n",
      "Ejemplo negativo: 0\n"
     ]
    }
   ],
   "source": [
    "pos_example = \"I loved this movie, it was amazing!\"\n",
    "neg_example = \"I hated this movie, it was terrible!\"\n",
    "\n",
    "print(f'Ejemplo positivo: {search.best_estimator_.predict([pos_example])[0]}')\n",
    "print(f'Ejemplo negativo: {search.best_estimator_.predict([neg_example])[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
