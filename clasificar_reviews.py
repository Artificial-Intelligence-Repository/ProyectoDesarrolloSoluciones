# -*- coding: utf-8 -*-
"""clasificar_reviews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q0XTonOEIzrP1Mf9O7Z85ciCYBGWFcsg

Importación de bibliotecas
"""

import os
import re
import tarfile
import zipfile
import pandas as pd
import numpy as np
import pickle
from sklearn.datasets import load_files
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import GridSearchCV
import scipy
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
import matplotlib.pyplot as plt
from sklearn.pipeline import Pipeline
# Naive Bayes
from sklearn.naive_bayes import MultinomialNB
# Linear regression
from sklearn.linear_model import LogisticRegression

"""Cambiar el workdir si se ejecuta el notebook localmente"""

# os.chdir(r"C:\Users\aeveg\OneDrive\Documentos\MASTERS\Proyecto\Proyecto de Materia\ProyectoDesarrolloSoluciones")

"""# Procesando los archivos del conjunto de datos

El conjunto de datos de reseñas de IMDB se encuentra constituido por una carpeta data que contiene dos carpetas, train y test, cada una con dos carpetas, pos y neg, que contienen 12500 archivos de texto con las reseñas de IMDB. Es necesario unificar estos archivos en un solo archivo CSV llamado imdb_reviews_train.csv y imdb_reviews_test.csv, respectivamente.
"""

def leer_archivos_en_directorio(directorio, sentimiento):
    """
    Lee todos los archivos de texto en 'directorio' y devuelve una lista de diccionarios,
    cada uno con el texto, el sentimiento (pos/neg) y si es train o test.
    """
    registros = []
    for filename in os.listdir(directorio):
        if filename.endswith(".txt"):
            ruta_archivo = os.path.join(directorio, filename)
            with open(ruta_archivo, "r", encoding="utf-8") as f:
                texto = f.read()
            registros.append({
                "review": texto,
                "sentiment": sentimiento,  # 1 = positivo, 0 = negativo
                #"split": tipo              # 'train' o 'test'
            })
    return registros

# Rutas (ajusta según tu sistema)
ruta_train_pos = "data/train/pos"
ruta_train_neg = "data/train/neg"
ruta_test_pos  = "data/test/pos"
ruta_test_neg  = "data/test/neg"

# Leemos los archivos de cada carpeta
train_pos = leer_archivos_en_directorio(ruta_train_pos, sentimiento=1)
train_neg = leer_archivos_en_directorio(ruta_train_neg, sentimiento=0)
test_pos  = leer_archivos_en_directorio(ruta_test_pos,  sentimiento=1)
test_neg  = leer_archivos_en_directorio(ruta_test_neg,  sentimiento=0)

# Convertimos las listas de datos de train y test a un DataFrame de pandas
train_df = pd.DataFrame(train_pos + train_neg)
test_df = pd.DataFrame(test_pos + test_neg)

# Desordenamos aleatoriamente los DataFrames
train_df = train_df.sample(frac=1, random_state=42).reset_index(drop=True)
test_df = test_df.sample(frac=1, random_state=42).reset_index(drop=True)

# Verificamos un poco la estructura de la data de entrenamiento
print("Verificando estructura de data de entrenamiento")
print(train_df.head())
print(train_df.tail())
print("Total de reseñas de entrenamiento:", len(train_df))

# Exportamos a CSV (sin índice)
train_df.to_csv("data/imdb_reviews_train.csv", index=False, encoding="utf-8")

# Verificamos un poco la estructura de la data de prueba
print("Verificando estructura de data de prueba")
print(test_df.head())
print(test_df.tail())
print("Total de reseñas de prueba:", len(test_df))

# Exportamos a CSV (sin índice)
test_df.to_csv("data/imdb_reviews_test.csv", index=False, encoding="utf-8")

"""Obteniendo data de entrenamiento"""

def load_imdb_reviews(ruta_archivo):
    # Lee el archivo CSV y lo guarda en un DataFrame
    df = pd.read_csv(ruta_archivo)
    return df

#df_train = load_imdb_reviews('./data/imdb_reviews_train_espanol.csv')
df_train = load_imdb_reviews('./data/imdb_reviews_train.csv')
print("\nPrimeras 5 filas del conjunto de datos de entrenamiento de reseñas de IMDB en en español:")
df_train.head()

df_train.groupby('sentiment').size()

"""Imprimiendo 5 ejemplos de la clase negativa"""

for review in np.array(df_train)[np.array(df_train.sentiment) == 0][:5]:
    print(review)

"""Imprimiendo 5 ejemplos de la clase positiva"""

for review in np.array(df_train)[np.array(df_train.sentiment) == 1][:5]:
    print(review)

"""## Procesamiento de texto

Inicialmente crearemos una función para procesar y tokenizar el texto con las siguientes características:

* Transformar el texto a minúsculas.
* Eliminar caracteres que no sean letras. Para este problema de clasificación no parecen ser relevantes los caracteres puntuación, los números, ni caracteres especiales.
* Solo conservamos tokens que contengan dos o más letras.
"""

def tokenize_text(text: str) -> "list[str]":
    """
    Procesa un texto y luego lo tokeniza.

    Args:
        text(str) texto a procesar y tokenizar.

    Returns:
        list[str] lista de tokens.
    """
    text = re.sub(r'[^a-z]', ' ', text.lower()) # Convertir a minúsculas y eliminar caracteres no alfabéticos
    text = re.sub(r'\s[a-z]([a-z])?\b', '', text) # Eliminar palabras de una o dos letras
    return text.split() # Tokenizar


sample = df_train.iloc[0].review
print('Texto original:')
print(sample)
print('Texto procesado y tokenizado:')
print(tokenize_text(sample))

"""Podemos constuir el vocabulario de nuestro corpus de entrenamiento. Este tamaño sería el tamaño que tendría la representación de bolsa de palabras para cada uno de los documentos:"""

vocabulary = {}
for r in np.array(df_train):
    #print(r[0])
    for token in tokenize_text(r[0]):
        vocabulary[token] = vocabulary.get(token, 0) + 1

print(f'Número de tokens únicos: {len(vocabulary)}')

"""Podemos ver los tokens más frecuentes:"""

# Ver los 10 tokens más frecuentes
sorted(vocabulary.items(), key=lambda x: x[1], reverse=True)[:10]

"""Estas tokens no parecen estar relacionados con el sentimiento de la reseña, por lo que podríamos eliminar los más frecuentes."""

sorted(vocabulary.items(), key=lambda x: x[1])[:10]

"""Por otro lado los tokens menos frecuentes parecen ser nombres de películas, personajes, actores, entre otros, que tampoco parecen estar relacionados con la tarea de clasificación de sentimientos.

# 2. Uso de Lexicones

Adicionalmente a las representaciones de bolsa de palabras vistas en semanas anteriores. Podemos construir otro tipo de características que nosotros o expertos en el dominio de la tarea consideren relevantes para la solución del problema.

En el caso de análisis de sentimiento existen diferentes lexicones en los que podemos encontrar la connotación negativa o positiva de las palabras. En esta sección utilizaremos el **Lexico Afinn del usuario de github jboscomendoza** que puede descargarse [aquí](https://raw.githubusercontent.com/jboscomendoza/lexicos-nrc-afinn/refs/heads/master/lexico_afinn.csv).

Primero extraigamos los archivos correspondientes al lexicon descargado.
"""

#lexicon = pd.read_csv('./data/lexico_afinn.csv')
#lexicon.head()

with open('data/subjclueslen1-HLTEMNLP05.tff') as f:
    lexicon = f.readlines()

for line in lexicon[:5]:
    print(line)

"""Según la documentación del lexicon, se resaltan las siguientes características:

* palabra: Contiene la palabra en español.
* puntuacion: Contiene la puntuación de la palabra en español que corresponde a la polaridad de la palabra, ya sea negativa, positiva o neutral, representada por numeros enteros que pueden ser positivos negativos o cero y cuyo valor indica mayor o menor subjetividad en el contexto.
* word: Contiene la palabra en inglés.

Uno de los grupos más simples de características que podemos extraer sobre el texto es la cantidad de palabras negativas y positivas que contiene. Así que primero podemos almacener de una manera conveniente los sets de palabras negativas y positivas contenidas en el lexicón.
"""

#negative_words = set()
#positive_words = set()

# Iteramos sobre las filas del lexicon
#for line in np.array(lexicon):
    # Extraemos la palabra y la polaridad de la línea
    #word = re.search(r'word1=(\w+)', line).group(1)
#    word = line[0]
#    polarity = line[1]
    # Guardamos la palabra en el conjunto correspondiente
#    if polarity <= 0:
#        negative_words.add(word)
#    else:
#print("Cantidad de palabras positivas:", len(positive_words))
#print("Cantidad de palabras negativas:", len(negative_words))
# Ver las primeras 10 palabras positivas
#print(f'10 palabras positivas: {list(positive_words)[:10]}')
# Ver las primeras 10 palabras negativas
#print(f'10 palabras negativas: {list(negative_words)[:10]}')

negative_words = set()
positive_words = set()

# Iteramos sobre las líneas del lexicon
for line in lexicon:
    # Extraemos la palabra y la polaridad de la línea
    word = re.search(r'word1=(\w+)', line).group(1)
    polarity = re.search(r'priorpolarity=(\w+)', line).group(1)
    # Guardamos la palabra en el conjunto correspondiente
    if polarity == 'negative':
        negative_words.add(word)
    if polarity == 'positive':
        positive_words.add(word)

print(f'Número de palabras positivas: {len(positive_words)}')
print(f'Número de palabras negativas: {len(negative_words)}')

# Ver las primeras 10 palabras positivas
print(f'10 palabras positivas: {list(positive_words)[:10]}')
# Ver las primeras 10 palabras negativas
print(f'10 palabras negativas: {list(negative_words)[:10]}')

"""Construyamos una clase similar a `CountVectorizer`de **scikit-learn** que nos permita extraer las características que definamos para cada documento del corpus.

En este ejemplo vamos a extraer únicamente el número de palabras positivas y negativas contenidas en cada documento según el lexicón que estamos usando.
"""

class LexiconVectorizer:
    def __init__(self, positive_words: set, negative_words: set):
        """
        Este vectorizador cuenta la cantidad de palabras positivas y negativas en un texto según los sets de palabras positivas y negativas con que se inicialice.

        Args:
            positive_words(set[str]): conjunto de palabras positivas.
            negative_words(set[str]): conjunto de palabras negativas.
        """
        self.positive_words = positive_words
        self.negative_words = negative_words

    # Esta vectorizador no utiliza los datos de entrenamiento, por lo que el método fit solo retorna self
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        """
        Este método transforma una lista de textos en una matriz de numpy donde cada fila es la cantidad de palabras positivas y negativas en el texto correspondiente.

        Args:
            X(list[str]): lista de textos a transformar.

        Returns:
            np.array: matriz de numpy con la cantidad de palabras positivas y negativas en cada texto.
        """
        docs = []
        for doc in X:
            docs.append(self.count_words(doc))

        return np.array(docs)

    def count_words(self, text: str) -> "list[int]":
        """
        Este método cuenta la cantidad de palabras positivas y negativas en un texto.

        Args:
            text(str): texto a procesar.

        Returns:
            list[int]: lista con la cantidad de palabras positivas y negativas en el texto.
        """
        proccessed_text = tokenize_text(text)
        positive_count = sum([1 for word in proccessed_text if word in self.positive_words])
        negative_count = sum([1 for word in proccessed_text if word in self.negative_words])

        return [positive_count, negative_count]

    def fit_transform(self, X, y=None):
        self.fit(X)
        return self.transform(X)

    def __repr__(self) -> str:
        return "LexiconVectorizer()"

lexicon_vectorizer = LexiconVectorizer(positive_words, negative_words)
X_train_lexicon = lexicon_vectorizer.fit_transform(df_train["review"])
print('Representación vectorial de las primeras 5 reseñas:')
print(X_train_lexicon[:5])

bow_vectorizer = CountVectorizer()
X_train_bow = bow_vectorizer.fit_transform(df_train["review"])
print('Representación vectorial de la primera reseña:')
X_train_bow[0]

"""La representación de CountVectorizer consiste en una matriz dispersa de 462 dimensiones, con 240 dimensiones con un valor mayor a 0.

Nosotros también podemos crear una representación de texto personalizada en la que usemos la tradicional bolsa de palabras junto con las características extraídas de los lexicones.
"""

class LexiconCountVectorizer(LexiconVectorizer):
    def __init__(self, positive_words: set, negative_words: set, *args, **kwargs):
        """
        Este vectorizador utiliza la representación generada por CountVectorizer junto con dos características sobre la proporción de palabras positivas y negativas identificadas en el documento.

        Args:
            positive_words(set[str]): conjunto de palabras positivas.
            negative_words(set[str]): conjunto de palabras negativas.
        """
        # Se inicializa la clase LexiconVectorizer
        super().__init__(positive_words, negative_words)
        # Se inicializa el CountVectorizer
        self.count_vect = CountVectorizer(*args, **kwargs)

    def fit(self, X, y=None):
        """
        Este método ajusta el vectorizador a los datos de entrenamiento.

        Args:
            X(list[str]): lista de textos de entrenamiento.
            y: no se utiliza.

        Returns:
            LexiconCountVectorizer: retorna el objeto actual.
        """
        # Se ajusta el CountVectorizer a los datos de entrenamiento
        self.count_vect.fit(X)
        return self

    def transform(self, X):
        """
        Este método transforma una lista de textos en una matriz de numpy donde cada fila contiene las características extraídas de cada documento.

        Args:
            X(list[str]): lista de textos a transformar.

        Returns:
            np.array: matriz de numpy con la representación vectorial de cada documento.
        """
        # Generar la representación de bolsa de palabras de los documentos
        bow = self.count_vect.transform(X)

        # Generar las características de palabras positivas y negativas de los documentos
        lexicon_features = []
        for doc in X:
            lexicon_features.append(self.count_words(doc))

        # Se cálcula la proporción de palabras positivas y negativas en cada documento
        lexicon_features = np.array(lexicon_features) / (np.sum(lexicon_features, axis=1).reshape(-1, 1) + 0.01) # Se suma 0.01 para evitar divisiones por cero
        # Se convierte a una matriz dispersa del mismo tipo que la matriz de bolsa de palabras
        lexicon_features = scipy.sparse.csr_matrix(lexicon_features)

        # Se concatenan las características de bolsa de palabras y las características de palabras positivas y negativas
        return scipy.sparse.hstack([lexicon_features, bow], format='csr')

    # set_params es un método utilizado por GridSearchCV para ajustar los parámetros del vectorizador
    def set_params(self, **params):
        """
        Este método ajusta los parámetros del CountVectorizer.

        Args:
            **params: parámetros a ajustar.

        Returns:
            LexiconCountVectorizer: retorna el objeto actual.
        """
        self.count_vect.set_params(**params)
        return self

    def __repr__(self) -> str:
        return "LexiconCountVectorizer()"

# Prueba del vectorizador
bow_vectorizer = LexiconCountVectorizer(positive_words, negative_words)
X_train_bow = bow_vectorizer.fit_transform(df_train["review"])
print('Representación vectorial de la primera reseña:')
X_train_bow[0]

"""Note como el resultado del vectorizador para la primera reseña tiene 2 dimensiones más que la representación de bolsa de palabras realizada en la celda anterior con `CountVectorizer`, y además la matriz dispersa contiene ahora 94 valores mayores a 0 en vez de 92.

Ahora observe los valores contenidos en la representación vectorial del primer documento del corpus:
"""

print(X_train_bow[0])

"""# 3. Entrenamiento de clasificadores de texto

En esta sección usaremos los clasificadores de **regresión logística** y **naive bayes** para clasificar el sentimiento de nuestras reseñas de películas. Vamos a probar distintas representaciones de texto para encontrar el modelo que mejor se ajusta a nuestro problema. Para realizar estas pruebas de manera conveniente vamos a utilizar `GridSearch` que nos permite poner a prueba el desempeño de distintos pipelines, y distintos hiperparámetros. Adicionalmente `GridSearch` utiliza validación cruzada para medir el desempeño de los modelos de una manera más robusta.

Consulte la siguiente guía para conocer otras técnicas de ajuste de hiperparámetros, y para entender mejor su funcionamiento. [Link a la guía](https://scikit-learn.org/stable/modules/grid_search.html#tips-for-parameter-search).

**Advertencia**: La ejecución de esta celda puede tardar más de 30 minutos.
"""

list_train_reviews = list(df_train["review"])
list_train_labels = list(df_train["sentiment"])

# Definimos la secuencia de pasos del pipeline que en este caso consiste en un vectorizador y un clasificador
pipeline = Pipeline([
    ('representation', CountVectorizer()),
    ('classifier', LogisticRegression())
])

# Definimos el espacio de búsqueda para cada paso del pipeline
# Definimos el espacio de búsqueda como una lista de diccionarios si queremos probar diferentes hipérparámetros por configuración
param_grid = [
    # Probamos diferentes configuraciones  y adicionalmente probamos diferentes hipérparámetros para los vectorizadores
    {
    'representation': [CountVectorizer(), TfidfVectorizer(), LexiconCountVectorizer(positive_words, negative_words)],
    'representation__max_df': [0.8, 0.9, 1.0],
    'representation__min_df': [0.1, 0.5, 1],
    'classifier': [MultinomialNB(), LogisticRegression(max_iter=1000)],
    }
]

# Realizamos la búsqueda de hiperparámetros
search = GridSearchCV(pipeline, param_grid, cv=3, scoring='accuracy', n_jobs=-1)
search.fit(list_train_reviews, list_train_labels)

print("Mejor configuración:\n{}\n".format(search.best_params_))
print("Mejor puntaje de validación cruzada: {:.2f}".format(search.best_score_))

"""La busqueda de hiperparámetros nos indica que la configuración que mejor resultados en promedio obtuvo fue la representación generada por `TfidfVectorizer`, en la que se utilizan todas las palabras del vocabulario ('representation__max_df': 1.0, y 'representation__min_df': 0) con un clasificador de regresión logística. En promedio esta configuración obtuvo un 89% de accuracy en las 5 particiones de evaluación generadas.

Adicionalmente, la clase `GridSearchCV` posee el atributo `cv_results_` en donde podemos ver todos los resultados obtenidos para cada configuración probada. Este diccionario puede leerse fácilmente como un `DataFrame` de **pandas**, y darnos información importante como la duración del entrenamiento (en segundos), los parámetros probados en la configuración, y los puntajes obtenidos de entrenamiento y de evaluación por cada uno de las particiones realizadas de los datos.
"""

# Mostramos los resultados de la búsqueda de hiperparámetros ordenados por el puntaje de validación cruzada promedio de mayor a menor
df_results = pd.DataFrame(search.cv_results_)
df_results.sort_values(by='mean_test_score', ascending=False)

"""En la tabla anterior se evidencia que en general los modelos de Regresión Logística obtienen un mejor desemepeño que los modelos de Naive Bayes. También se observa que en general las mejores representaciones de texto son `TfidfVectorizer`, `LexiconCountVectorizer`, `CountVectorizer`, y `LexiconVectorizer` en el orden correspondiente, y finalmente se observa que para los vectorizadores los mejores hiperpárametros son utilizar todas las palabras presentes en el corpus a excepción de aquellas con una única ocurrencia.

# 4. Evaluación de los clasificadores

Finalmente, hacemos uso del set de evaluación para validar el desempeño de nuestro modelo.

En primer lugar, cargamos el set de evaluación:
"""

df_test = load_imdb_reviews('./data/imdb_reviews_test.csv')
print("\nPrimeras 5 filas del conjunto de datos de prueba de reseñas de IMDB en en español:")
df_test.head()

"""Ahora calculamos el accuracy del modelo con mejor desempeño:"""

search.best_estimator_.score(df_test["review"], df_test["sentiment"])

"""También podemos ver la matriz de confusión generada por nuestro modelo en el set evaluación:"""

# Realizamos la predicción sobre el conjunto de prueba
y_pred = search.best_estimator_.predict(df_test["review"])

# Construimos la matriz de confusión
conf_mtx = confusion_matrix(df_test["sentiment"], y_pred, normalize='true')

# Mostramos la matriz de confusión
disp = ConfusionMatrixDisplay(conf_mtx, display_labels=["neg","pos"])
disp.plot()
plt.show()

"""Otras métricas que pueder ser relevantes según el problema son el `f1-score`, la precisión, y la sensitividad, ya sean macro o micro:"""

print(classification_report(df_test["sentiment"], y_pred, target_names=["neg", "pos"]))

"""Veamos ejemplos de clasificación de nuestro modelo:"""

pos_example = "I loved this movie, it was amazing!"
neg_example = "I hated this movie, it was terrible!"

print(f'Ejemplo positivo: {search.best_estimator_.predict([pos_example])[0]}')
print(f'Ejemplo negativo: {search.best_estimator_.predict([neg_example])[0]}')

